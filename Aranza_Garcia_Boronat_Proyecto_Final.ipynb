{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "contained-norman",
   "metadata": {},
   "source": [
    "# Proyecto Final - ANALITICA BIG DATA - Modelo predicción lluvia Australia \n",
    "\n",
    "### Realizado por: ARANZA GARCÍA BORONAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "usual-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd \n",
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from pyspark.sql import SparkSession\n",
    "# create sparksession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Pysparkexample\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "therapeutic-minute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-calibration",
   "metadata": {},
   "source": [
    "### Lectura de dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "affecting-geometry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark\\\n",
    ".read\\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"delimiter\", \",\")\\\n",
    ".option(\"inferSchema\", True)\\\n",
    ".load(\"rain_tomorrow_in_australia_mod.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "characteristic-employer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " 'Location',\n",
       " 'MinTemp',\n",
       " 'MaxTemp',\n",
       " 'Rainfall',\n",
       " 'Evaporation',\n",
       " 'Sunshine',\n",
       " 'WindGustDir',\n",
       " 'WindGustSpeed',\n",
       " 'WindDir9am',\n",
       " 'WindDir3pm',\n",
       " 'WindSpeed9am',\n",
       " 'WindSpeed3pm',\n",
       " 'Humidity9am',\n",
       " 'Humidity3pm',\n",
       " 'Pressure9am',\n",
       " 'Pressure3pm',\n",
       " 'Cloud9am',\n",
       " 'Cloud3pm',\n",
       " 'Temp9am',\n",
       " 'Temp3pm',\n",
       " 'RainToday',\n",
       " 'RISK_MM',\n",
       " 'RainTomorrow',\n",
       " 'PXW']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-evolution",
   "metadata": {},
   "source": [
    "Ver el total de columnas y tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "smaller-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- PXW: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-disorder",
   "metadata": {},
   "source": [
    "Ver el total de registros así como comprobar que no hay duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "right-avenue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142193"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "illegal-childhood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142193"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bigger-whole",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+----+\n",
      "|      Date|Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RISK_MM|RainTomorrow| PXW|\n",
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+----+\n",
      "|2008-12-01|  Albury|   13.4|   22.9|     0.6|       null|    null|          W|         44.0|         W|       WNW|        20.0|        24.0|       71.0|       22.0|     1007.7|     1007.1|     8.0|    null|   16.9|   21.8|       No|    0.0|          No|12.4|\n",
      "|2008-12-02|  Albury|    7.4|   25.1|     0.0|       null|    null|        WNW|         44.0|       NNW|       WSW|         4.0|        22.0|       44.0|       25.0|     1010.6|     1007.8|    null|    null|   17.2|   24.3|       No|    0.0|          No|12.4|\n",
      "|2008-12-03|  Albury|   12.9|   25.7|     0.0|       null|    null|        WSW|         46.0|         W|       WSW|        19.0|        26.0|       38.0|       30.0|     1007.6|     1008.7|    null|     2.0|   21.0|   23.2|       No|    0.0|          No|12.4|\n",
      "|2008-12-04|  Albury|  139.2|   28.0|     0.0|       null|    null|         NE|         24.0|        SE|         E|        11.0|         9.0|       45.0|       16.0|     1017.6|     1012.8|    null|    null|   18.1|   26.5|       No|    1.0|          No|12.4|\n",
      "|2008-12-05|  Albury|   17.5|   32.3|     1.0|       null|    null|          W|         41.0|       ENE|        NW|         7.0|        20.0|       82.0|       33.0|     1010.8|     1006.0|     7.0|     8.0|   17.8|   29.7|       No|    0.2|          No|12.4|\n",
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-model",
   "metadata": {},
   "source": [
    "Realizo un describe que ya me permite ver el número de nulos por columnas, valores medios, desviación típica, mínimos y máximos, con el fin de tener una visión general de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "headed-diamond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|          MinTemp|          MaxTemp|          Rainfall|      Evaporation|          Sunshine|     WindGustSpeed|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|           141552|           141867|            140787|            81350|             74377|            132919|\n",
      "|   mean|12.18742158358772|23.23730959278764|2.3499740743107385|5.469824216349125| 7.624853113193599|39.984103100384445|\n",
      "| stddev|6.412181910203625|7.941855439811297| 8.465172917616467|4.188536508895161|3.7815249942144544|13.588649294637493|\n",
      "|    min|             -8.5|             -4.8|               0.0|              0.0|               0.0|               6.0|\n",
      "|    max|            139.2|           1333.4|             371.0|            145.0|              14.5|             135.0|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"MinTemp\", \"MaxTemp\",\"Rainfall\", \"Evaporation\", \"Sunshine\",\"WindGustSpeed\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "afraid-warrior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|     WindSpeed9am|     WindSpeed3pm|      Humidity9am|       Humidity3pm|       Pressure9am|       Pressure3pm|\n",
      "+-------+-----------------+-----------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|           140845|           139563|           140419|            138583|            128179|            128212|\n",
      "|   mean|  14.001988000994|18.63757586179718| 68.8438103105705|51.482606091656265|1017.6537584159634|1015.2582035378888|\n",
      "| stddev|8.893337098234488|8.803345036235546|19.05129253533625|20.797771843698854|  7.10547571152079| 7.036676783493588|\n",
      "|    min|              0.0|              0.0|              0.0|               0.0|             980.5|             977.1|\n",
      "|    max|            130.0|             87.0|            100.0|             100.0|            1041.0|            1039.6|\n",
      "+-------+-----------------+-----------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('WindSpeed9am',\n",
    " 'WindSpeed3pm',\n",
    " 'Humidity9am',\n",
    " 'Humidity3pm',\n",
    " 'Pressure9am',\n",
    " 'Pressure3pm').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "adolescent-scenario",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|         Cloud9am|         Cloud3pm|           Temp9am|          Temp3pm|           RISK_MM|               PXW|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|            88536|            85099|            141289|           139467|            142193|            142193|\n",
      "|   mean|4.437189391885787|4.503166899728551|16.987508581701338|21.68723497314774|2.3606816087992066|12.400000000006994|\n",
      "| stddev|  2.8870155257336|2.720632530403663|  6.49283832547889| 6.93759386853373|  8.47796906922771|               0.0|\n",
      "|    min|              0.0|              0.0|              -7.2|             -5.4|               0.0|              12.4|\n",
      "|    max|              9.0|              9.0|              40.2|             46.7|             371.0|              12.4|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('Cloud9am',\n",
    " 'Cloud3pm',\n",
    " 'Temp9am',\n",
    " 'Temp3pm',\n",
    " 'RISK_MM',\n",
    " 'PXW').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-grenada",
   "metadata": {},
   "source": [
    "Eliminamos tal como se indican en las instrucciones de entrada las siguientes columnas: \"RISK_MM\", \"Date\" and \"Location\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "spectacular-radar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- PXW: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = df.drop(col('Date')).drop(col('Location')).drop(col('RISK_MM'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-elimination",
   "metadata": {},
   "source": [
    "Aplico función para ver el número de nulos de las diferentes columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "measured-cursor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------------+-----------------+--------------+-----------------+-------------------+----------------+----------------+------------------+------------------+-----------------+-----------------+-----------------+-----------------+--------------+--------------+-------------+-------------+---------------+------------------+---------+\n",
      "|MinTemp_nulls|MaxTemp_nulls|Rainfall_nulls|Evaporation_nulls|Sunshine_nulls|WindGustDir_nulls|WindGustSpeed_nulls|WindDir9am_nulls|WindDir3pm_nulls|WindSpeed9am_nulls|WindSpeed3pm_nulls|Humidity9am_nulls|Humidity3pm_nulls|Pressure9am_nulls|Pressure3pm_nulls|Cloud9am_nulls|Cloud3pm_nulls|Temp9am_nulls|Temp3pm_nulls|RainToday_nulls|RainTomorrow_nulls|PXW_nulls|\n",
      "+-------------+-------------+--------------+-----------------+--------------+-----------------+-------------------+----------------+----------------+------------------+------------------+-----------------+-----------------+-----------------+-----------------+--------------+--------------+-------------+-------------+---------------+------------------+---------+\n",
      "|          641|          326|          1406|            60843|         67816|             9334|               9274|           10013|            3778|              1348|              2630|             1774|             3610|            14014|            13981|         53657|         57094|          904|         2726|           1406|                 0|        0|\n",
      "+-------------+-------------+--------------+-----------------+--------------+-----------------+-------------------+----------------+----------------+------------------+------------------+-----------------+-----------------+-----------------+-----------------+--------------+--------------+-------------+-------------+---------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as sql_sum, col\n",
    "\n",
    "all_cols=df.columns\n",
    "\n",
    "null_counts = [sql_sum(col(c).isNull().cast(\"int\")).alias(c + \"_nulls\") for c in all_cols]\n",
    "\n",
    "df.select(*null_counts).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-roulette",
   "metadata": {},
   "source": [
    "Defino función para sustituir los nulos por las medias de cada columna a excepción de las columnas tipo String que realizaré en el siguiente paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "altered-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "def fill_with_mean(df, exclude=set()): \n",
    "    stats = df.agg(*(\n",
    "        avg(c).alias(c) for c in df.columns if c not in exclude\n",
    "    ))\n",
    "    return df.na.fill(stats.first().asDict())\n",
    "\n",
    "df = fill_with_mean(df, [\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\", \"RainToday\", \"RainTomorrow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "personal-marsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------------+-----------------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+-----------------+-----------------+-------+-------+---------+------------+----+\n",
      "|MinTemp|MaxTemp|Rainfall|      Evaporation|         Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|         Cloud9am|         Cloud3pm|Temp9am|Temp3pm|RainToday|RainTomorrow| PXW|\n",
      "+-------+-------+--------+-----------------+-----------------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+-----------------+-----------------+-------+-------+---------+------------+----+\n",
      "|   13.4|   22.9|     0.6|5.469824216349125|7.624853113193599|          W|         44.0|         W|       WNW|        20.0|        24.0|       71.0|       22.0|     1007.7|     1007.1|              8.0|4.503166899728551|   16.9|   21.8|       No|          No|12.4|\n",
      "|    7.4|   25.1|     0.0|5.469824216349125|7.624853113193599|        WNW|         44.0|       NNW|       WSW|         4.0|        22.0|       44.0|       25.0|     1010.6|     1007.8|4.437189391885787|4.503166899728551|   17.2|   24.3|       No|          No|12.4|\n",
      "|   12.9|   25.7|     0.0|5.469824216349125|7.624853113193599|        WSW|         46.0|         W|       WSW|        19.0|        26.0|       38.0|       30.0|     1007.6|     1008.7|4.437189391885787|              2.0|   21.0|   23.2|       No|          No|12.4|\n",
      "|  139.2|   28.0|     0.0|5.469824216349125|7.624853113193599|         NE|         24.0|        SE|         E|        11.0|         9.0|       45.0|       16.0|     1017.6|     1012.8|4.437189391885787|4.503166899728551|   18.1|   26.5|       No|          No|12.4|\n",
      "|   17.5|   32.3|     1.0|5.469824216349125|7.624853113193599|          W|         41.0|       ENE|        NW|         7.0|        20.0|       82.0|       33.0|     1010.8|     1006.0|              7.0|              8.0|   17.8|   29.7|       No|          No|12.4|\n",
      "|   14.6|   29.7|     0.2|5.469824216349125|7.624853113193599|        WNW|         56.0|         W|         W|        19.0|        24.0|       55.0|       23.0|     1009.2|     1005.4|4.437189391885787|4.503166899728551|   20.6|   28.9|       No|          No|12.4|\n",
      "|   14.3|   25.0|     0.0|5.469824216349125|7.624853113193599|          W|         50.0|        SW|         W|        20.0|        24.0|       49.0|       19.0|     1009.6|     1008.2|              1.0|4.503166899728551|   18.1|   24.6|       No|          No|12.4|\n",
      "|    7.7|   26.7|     0.0|5.469824216349125|7.624853113193599|          W|         35.0|       SSE|         W|         6.0|        17.0|       48.0|       19.0|     1013.4|     1010.1|4.437189391885787|4.503166899728551|   16.3|   25.5|       No|          No|12.4|\n",
      "|    9.7|   31.9|     0.0|5.469824216349125|7.624853113193599|        NNW|         80.0|        SE|        NW|         7.0|        28.0|       42.0|        9.0|     1008.9|     1003.6|4.437189391885787|4.503166899728551|   18.3|   30.2|       No|         Yes|12.4|\n",
      "|   13.1|   30.1|     1.4|5.469824216349125|7.624853113193599|          W|         28.0|         S|       SSE|        15.0|        11.0|       58.0|       27.0|     1007.0|     1005.7|4.437189391885787|4.503166899728551|   20.1|   28.2|      Yes|          No|12.4|\n",
      "+-------+-------+--------+-----------------+-----------------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+-----------------+-----------------+-------+-------+---------+------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-condition",
   "metadata": {},
   "source": [
    "Por último en esta preparación y limpieza de datos sustituyo en las variables tipo String los valores vacíos por \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "authentic-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill(value=\"unknown\",subset=[\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\", \"RainToday\", \"RainTomorrow\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-semester",
   "metadata": {},
   "source": [
    "Compruebo que no quedan nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "clean-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = false)\n",
      " |-- MaxTemp: double (nullable = false)\n",
      " |-- Rainfall: double (nullable = false)\n",
      " |-- Evaporation: double (nullable = false)\n",
      " |-- Sunshine: double (nullable = false)\n",
      " |-- WindGustDir: string (nullable = false)\n",
      " |-- WindGustSpeed: double (nullable = false)\n",
      " |-- WindDir9am: string (nullable = false)\n",
      " |-- WindDir3pm: string (nullable = false)\n",
      " |-- WindSpeed9am: double (nullable = false)\n",
      " |-- WindSpeed3pm: double (nullable = false)\n",
      " |-- Humidity9am: double (nullable = false)\n",
      " |-- Humidity3pm: double (nullable = false)\n",
      " |-- Pressure9am: double (nullable = false)\n",
      " |-- Pressure3pm: double (nullable = false)\n",
      " |-- Cloud9am: double (nullable = false)\n",
      " |-- Cloud3pm: double (nullable = false)\n",
      " |-- Temp9am: double (nullable = false)\n",
      " |-- Temp3pm: double (nullable = false)\n",
      " |-- RainToday: string (nullable = false)\n",
      " |-- RainTomorrow: string (nullable = false)\n",
      " |-- PXW: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "scheduled-calendar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|RainTomorrow|count(1)|\n",
      "+------------+--------+\n",
      "|          No|  110316|\n",
      "|         Yes|   31877|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.groupBy('RainTomorrow').agg(count('*')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-spring",
   "metadata": {},
   "source": [
    "Preparación del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "green-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a utilizar \"filters methods\" es el más sencillo, al tener 21 variables tenemos un espacio de dimensión 21, por lo que vamos\n",
    "# a intentar reducir variables con un \"AUC\", área bajo la curva ROC, nos quedamos con las que mayor capacidad predictiva tenga.\n",
    "#AUC como de bueno es un modelo para clasificación binaria, por lo que vamos a resolver el problema una a una variable y veo\n",
    "#las que mejor AUC me da para así hacer selcción de características. AUC: TPR vs FPR (Sensibilidad vs 1 - Especificidad)\n",
    "#AUC=P(fa|1 > fa|0), AUC ha de ser próximo a 1. Si tengo 0,5 es totalmente aleatorio se solapan 0 y 1 totalmente, si es menor que 0,5, es mejor darle la vuelta al clasificador.\n",
    "\n",
    "def get_auc(df, c):\n",
    "\n",
    "    from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "    preds_and_labels = df\\\n",
    "    .select(c, 'RainTomorrow')\\\n",
    "    .withColumn('label', when(col('RainTomorrow')=='Yes', 1.0).otherwise(0.0))\\\n",
    "    .rdd\\\n",
    "    .map(lambda r: (r[c], r['label']))\n",
    "\n",
    "    auc = BinaryClassificationMetrics(preds_and_labels).areaUnderROC\n",
    "\n",
    "    print(str(auc))\n",
    "\n",
    "    return (c, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "alive-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_cols = [e[0] for e in df.dtypes if e[1]==\"double\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ranging-criticism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5527071867293107\n",
      "0.39020806285961396\n",
      "0.6998593428598958\n",
      "0.4368569559180371\n",
      "0.29881251958436095\n",
      "0.6414502213760991\n",
      "0.5559240717710611\n",
      "0.5520068301838192\n",
      "0.6832462038176417\n",
      "0.7929080299419458\n",
      "0.34735920011459714\n",
      "0.3605525326740113\n",
      "0.667928573412419\n",
      "0.6975133431976344\n",
      "0.47912771555904243\n",
      "0.3683303549481391\n",
      "0.5\n",
      "+-------------+-------------------+\n",
      "|         feat|                auc|\n",
      "+-------------+-------------------+\n",
      "|  Humidity3pm| 0.7929080299419458|\n",
      "|     Rainfall| 0.6998593428598958|\n",
      "|     Cloud3pm| 0.6975133431976344|\n",
      "|  Humidity9am| 0.6832462038176417|\n",
      "|     Cloud9am|  0.667928573412419|\n",
      "|WindGustSpeed| 0.6414502213760991|\n",
      "| WindSpeed9am| 0.5559240717710611|\n",
      "|      MinTemp| 0.5527071867293107|\n",
      "| WindSpeed3pm| 0.5520068301838192|\n",
      "|          PXW|                0.5|\n",
      "|      Temp9am|0.47912771555904243|\n",
      "|  Evaporation| 0.4368569559180371|\n",
      "|      MaxTemp|0.39020806285961396|\n",
      "|      Temp3pm| 0.3683303549481391|\n",
      "|  Pressure3pm| 0.3605525326740113|\n",
      "|  Pressure9am|0.34735920011459714|\n",
      "|     Sunshine|0.29881251958436095|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, desc\n",
    "\n",
    "stylepredcapdf = spark.createDataFrame([get_auc(df, c) for c in double_cols], ['feat', 'auc'])\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "stylepredcapdf.orderBy(desc('auc')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-honolulu",
   "metadata": {},
   "source": [
    "Con este dataframe ya vemos que la variable Humidity3pm con un 0.79 de AUC puede ser de gran ayuda en el modelo predictivo, las variables con AUC de 0.5 son como si fueran totalmente aleatorias, (como lanzar una modena al aire), y en el caso de las más bajitas hay que obtener la inversa para que nos sean de utilidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "alert-number",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|         feat|               auc|\n",
      "+-------------+------------------+\n",
      "|  Humidity3pm|0.7929080299419458|\n",
      "|     Sunshine| 0.701187480415639|\n",
      "|     Rainfall|0.6998593428598958|\n",
      "|     Cloud3pm|0.6975133431976344|\n",
      "|  Humidity9am|0.6832462038176417|\n",
      "|     Cloud9am| 0.667928573412419|\n",
      "|  Pressure9am|0.6526407998854029|\n",
      "|WindGustSpeed|0.6414502213760991|\n",
      "|  Pressure3pm|0.6394474673259887|\n",
      "|      Temp3pm|0.6316696450518609|\n",
      "|      MaxTemp| 0.609791937140386|\n",
      "|  Evaporation|0.5631430440819629|\n",
      "| WindSpeed9am|0.5559240717710611|\n",
      "|      MinTemp|0.5527071867293107|\n",
      "| WindSpeed3pm|0.5520068301838192|\n",
      "|      Temp9am|0.5208722844409576|\n",
      "|          PXW|               0.5|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feat_subset = stylepredcapdf\\\n",
    ".withColumn('auc', when(col('auc') < 0.5, 1 - col('auc')).otherwise(col('auc')))\\\n",
    ".orderBy(desc('auc')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-gospel",
   "metadata": {},
   "source": [
    "En nuestro caso nos vamos a quedar con las variables de AUC superior a 0.65, si cogieramos mñas de 0,75 tan solo construiriamos el modelo con una variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "welsh-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_subset = stylepredcapdf\\\n",
    ".withColumn('auc', when(col('auc') < 0.5, 1 - col('auc')).otherwise(col('auc')))\\\n",
    ".orderBy(desc('auc')).filter(col('auc') >= 0.65).select('feat')\\\n",
    ".rdd\\\n",
    ".map(lambda r: r['feat']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "handmade-blink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Humidity3pm',\n",
       " 'Sunshine',\n",
       " 'Rainfall',\n",
       " 'Cloud3pm',\n",
       " 'Humidity9am',\n",
       " 'Cloud9am',\n",
       " 'Pressure9am']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-device",
   "metadata": {},
   "source": [
    "# Parte 1 Ejercicio - Crear Modelo Predictivo para toda Australia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-insulation",
   "metadata": {},
   "source": [
    "# 1º Modelo con Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "oriented-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import sum as sql_sum, col, when, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "foster-hunter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = false)\n",
      " |-- MaxTemp: double (nullable = false)\n",
      " |-- Rainfall: double (nullable = false)\n",
      " |-- Evaporation: double (nullable = false)\n",
      " |-- Sunshine: double (nullable = false)\n",
      " |-- WindGustDir: string (nullable = false)\n",
      " |-- WindGustSpeed: double (nullable = false)\n",
      " |-- WindDir9am: string (nullable = false)\n",
      " |-- WindDir3pm: string (nullable = false)\n",
      " |-- WindSpeed9am: double (nullable = false)\n",
      " |-- WindSpeed3pm: double (nullable = false)\n",
      " |-- Humidity9am: double (nullable = false)\n",
      " |-- Humidity3pm: double (nullable = false)\n",
      " |-- Pressure9am: double (nullable = false)\n",
      " |-- Pressure3pm: double (nullable = false)\n",
      " |-- Cloud9am: double (nullable = false)\n",
      " |-- Cloud3pm: double (nullable = false)\n",
      " |-- Temp9am: double (nullable = false)\n",
      " |-- Temp3pm: double (nullable = false)\n",
      " |-- RainToday: string (nullable = false)\n",
      " |-- RainTomorrow: string (nullable = false)\n",
      " |-- PXW: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nonprofit-stomach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142193"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "artificial-motion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|RainTomorrow|count(1)|\n",
      "+------------+--------+\n",
      "|          No|  110316|\n",
      "|         Yes|   31877|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('RainTomorrow').agg(count('*')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-conspiracy",
   "metadata": {},
   "source": [
    "Hacemos las partición de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "computational-anaheim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99599\n",
      "42594\n"
     ]
    }
   ],
   "source": [
    "(tr_set, tt_set) = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "print(tr_set.count())\n",
    "\n",
    "\n",
    "print(tt_set.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-history",
   "metadata": {},
   "source": [
    "Si debemos completar los nulos lo hacemos sobre el conjunto de train.\n",
    "Ahora vamos a diseñar nuestras etapas del pipeline:\n",
    "1ª Assembler\n",
    "2ª Scaler, si elegimos un algoritmo que lo necesita, si es un RF o arbol de decisión no hace falta.\n",
    "2ª stringindexer =>df=df.withColumn('label_idx', when(col('label')=='famele'\n",
    "3ª RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "naughty-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [c[0] for c in df.dtypes if c[1]=='double']\n",
    "\n",
    "assembler = VectorAssembler()\\\n",
    ".setInputCols(feats)\\\n",
    ".setOutputCol(\"features\")\n",
    "\n",
    "# Defining the label/target column\n",
    "\n",
    "label_indexer = StringIndexer()\\\n",
    ".setInputCol(\"RainTomorrow\")\\\n",
    ".setOutputCol(\"label_idx\")\n",
    "\n",
    "# A random forest as classifier\n",
    "\n",
    "classstage = RandomForestClassifier()\\\n",
    "    .setNumTrees(50)\\\n",
    "    .setFeatureSubsetStrategy(\"sqrt\")\\\n",
    "    .setImpurity(\"gini\")\\\n",
    "    .setMaxDepth(16)\\\n",
    "    .setMaxBins(32)\\\n",
    "    .setLabelCol(\"label_idx\")\\\n",
    "    .setFeaturesCol(\"features\")\\\n",
    "    .setSubsamplingRate(0.7)\\\n",
    "    .setMinInstancesPerNode(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "recreational-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pipeline\n",
    "\n",
    "pipeline = Pipeline()\\\n",
    "    .setStages([assembler, label_indexer, classstage])\n",
    "\n",
    " # Training\n",
    "model1 = pipeline.fit(tr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "selected-packing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_15b5d71ebff0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "imperial-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Making preds on test data\n",
    "\n",
    "ttpreds = model1.transform(tt_set)\n",
    "\n",
    "trpreds = model1.transform(tr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "given-coordinate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info MyVoiceDetectionApp] ModelModelo con Random Forest Classifier - AUC(tr) = 0.9161648220063296 - AUC(tt) = 0.8739650875499182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "# Model evaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    ".setLabelCol(\"label_idx\")\\\n",
    ".setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "ttauc = evaluator.evaluate(ttpreds)\n",
    "\n",
    "trauc = evaluator.evaluate(trpreds)\n",
    "\n",
    "print(\"\\n[Info MyVoiceDetectionApp] ModelModelo con Random Forest Classifier - AUC(tr) = \" + str(trauc) + \" - AUC(tt) = \" + str(ttauc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-comparative",
   "metadata": {},
   "source": [
    "####  El mejor resultado de los diferentes modelos realizados, en  test AUC de 0.874"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-vacuum",
   "metadata": {},
   "source": [
    "### Realizo el mismo modelo Random Forest Classifier, pero en este caso no uso todas las variables doubles sino las que tienen un mayor AUC, calculadas en la reduccción de variable con método \"filters methods\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "arctic-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler()\\\n",
    ".setInputCols(feat_subset)\\\n",
    ".setOutputCol(\"features\")\n",
    "\n",
    "# Defining the label/target column\n",
    "\n",
    "label_indexer = StringIndexer()\\\n",
    ".setInputCol(\"RainTomorrow\")\\\n",
    ".setOutputCol(\"label_idx\")\n",
    "\n",
    "# A random forest as classifier\n",
    "\n",
    "classstage = RandomForestClassifier()\\\n",
    "    .setNumTrees(50)\\\n",
    "    .setFeatureSubsetStrategy(\"sqrt\")\\\n",
    "    .setImpurity(\"gini\")\\\n",
    "    .setMaxDepth(16)\\\n",
    "    .setMaxBins(32)\\\n",
    "    .setLabelCol(\"label_idx\")\\\n",
    "    .setFeaturesCol(\"features\")\\\n",
    "    .setSubsamplingRate(0.7)\\\n",
    "    .setMinInstancesPerNode(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "electoral-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pipeline\n",
    "\n",
    "pipeline = Pipeline()\\\n",
    "    .setStages([assembler, label_indexer, classstage])\n",
    "\n",
    " # Training\n",
    "model_R = pipeline.fit(tr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "inclusive-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Making preds on test data\n",
    "\n",
    "ttpreds = model_R.transform(tt_set)\n",
    "\n",
    "trpreds = model_R.transform(tr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "disturbed-margin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info MyVoiceDetectionApp] ModelModelo con Random Forest Classifier - AUC(tr) = 0.8844168724176003 - AUC(tt) = 0.853709958721402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "# Model evaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    ".setLabelCol(\"label_idx\")\\\n",
    ".setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "ttauc = evaluator.evaluate(ttpreds)\n",
    "\n",
    "trauc = evaluator.evaluate(trpreds)\n",
    "\n",
    "print(\"\\n[Info MyVoiceDetectionApp] ModelModelo con Random Forest Classifier - AUC(tr) = \" + str(trauc) + \" - AUC(tt) = \" + str(ttauc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-ebony",
   "metadata": {},
   "source": [
    "####  Empeoro el resultado en test, por lo que es mejor tener en cuenta todas las varibles. En este caso no son muchas, hay otros casos en los que se ha de construir el modelo con un gran número de variables y el método \"filters methods\" puede ser de gran ayuda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-thermal",
   "metadata": {},
   "source": [
    "# 2º Modelo con Gradient-Boosted Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-jacket",
   "metadata": {},
   "source": [
    "Aplico el modelo Gadient-Booster Tree Classifier para ver si mejoro el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "private-recall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = false)\n",
      " |-- MaxTemp: double (nullable = false)\n",
      " |-- Rainfall: double (nullable = false)\n",
      " |-- Evaporation: double (nullable = false)\n",
      " |-- Sunshine: double (nullable = false)\n",
      " |-- WindGustDir: string (nullable = false)\n",
      " |-- WindGustSpeed: double (nullable = false)\n",
      " |-- WindDir9am: string (nullable = false)\n",
      " |-- WindDir3pm: string (nullable = false)\n",
      " |-- WindSpeed9am: double (nullable = false)\n",
      " |-- WindSpeed3pm: double (nullable = false)\n",
      " |-- Humidity9am: double (nullable = false)\n",
      " |-- Humidity3pm: double (nullable = false)\n",
      " |-- Pressure9am: double (nullable = false)\n",
      " |-- Pressure3pm: double (nullable = false)\n",
      " |-- Cloud9am: double (nullable = false)\n",
      " |-- Cloud3pm: double (nullable = false)\n",
      " |-- Temp9am: double (nullable = false)\n",
      " |-- Temp3pm: double (nullable = false)\n",
      " |-- RainToday: string (nullable = false)\n",
      " |-- RainTomorrow: string (nullable = false)\n",
      " |-- PXW: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dangerous-healthcare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99208\n",
      "42985\n"
     ]
    }
   ],
   "source": [
    "(tr_set, tt_set) = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "print(tr_set.count())\n",
    "\n",
    "\n",
    "print(tt_set.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "nutritional-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "feats = [c[0] for c in df.dtypes if c[1]=='double']\n",
    "\n",
    "assembler = VectorAssembler()\\\n",
    ".setInputCols(feats)\\\n",
    ".setOutputCol(\"features\")\n",
    "\n",
    "# Defining the label/target column\n",
    "\n",
    "label_indexer = StringIndexer()\\\n",
    ".setInputCol(\"RainTomorrow\")\\\n",
    ".setOutputCol(\"label_idx\")\n",
    "\n",
    "# Gradient-Boosted Tree Classifier\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"label_idx\", featuresCol=\"features\", maxIter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "wooden-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pipeline\n",
    "\n",
    "pipeline2 = Pipeline()\\\n",
    "    .setStages([assembler, label_indexer, gbt])\n",
    "\n",
    " # Training\n",
    "model2 = pipeline2.fit(tr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "flexible-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Making preds on test data\n",
    "\n",
    "ttpreds = model2.transform(tt_set)\n",
    "\n",
    "trpreds = model2.transform(tr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "realistic-actor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info MyRainPrediction] Model Gradient-Boosted Tree Classifier - AUC(tr) = 0.8612484076650002 - AUC(tt) = 0.8599367982351445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "# Model evaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    ".setLabelCol(\"label_idx\")\\\n",
    ".setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "ttauc = evaluator.evaluate(ttpreds)\n",
    "\n",
    "trauc = evaluator.evaluate(trpreds)\n",
    "\n",
    "print(\"\\n[Info MyRainPrediction] Model Gradient-Boosted Tree Classifier - AUC(tr) = \" + str(trauc) + \" - AUC(tt) = \" + str(ttauc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-dutch",
   "metadata": {},
   "source": [
    "#### No mejoro el resultado de Random Forest Classifier con todas las variables doubles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-piece",
   "metadata": {},
   "source": [
    "# 3º Modelo con Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-examination",
   "metadata": {},
   "source": [
    "Aplico el modelo Logistic Regression para ver si mejoro el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "concerned-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99349\n",
      "42844\n"
     ]
    }
   ],
   "source": [
    "(tr_set, tt_set) = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "print(tr_set.count())\n",
    "\n",
    "\n",
    "print(tt_set.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "young-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "feats = [c[0] for c in df.dtypes if c[1]=='double']\n",
    "\n",
    "assembler = VectorAssembler()\\\n",
    ".setInputCols(feats)\\\n",
    ".setOutputCol(\"features\")\n",
    "\n",
    "# Defining the label/target column\n",
    "\n",
    "label_indexer = StringIndexer()\\\n",
    ".setInputCol(\"RainTomorrow\")\\\n",
    ".setOutputCol(\"label_idx\")\n",
    "\n",
    "# Logistic Regression Model\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label_idx\", featuresCol=\"features\", maxIter=10,regParam=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "incredible-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pipeline\n",
    "\n",
    "pipeline3 = Pipeline()\\\n",
    "    .setStages([assembler, label_indexer, lr])\n",
    "\n",
    " # Training\n",
    "model3 = pipeline3.fit(tr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "hydraulic-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Making preds on test data\n",
    "\n",
    "ttpreds = model3.transform(tt_set)\n",
    "\n",
    "trpreds = model3.transform(tr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "revolutionary-burns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info MyRainPrediction] Model Logistic Regression - AUC(tr) = 0.8211747079997799 - AUC(tt) = 0.8181858238623891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "# Model evaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    ".setLabelCol(\"label_idx\")\\\n",
    ".setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "ttauc = evaluator.evaluate(ttpreds)\n",
    "\n",
    "trauc = evaluator.evaluate(trpreds)\n",
    "\n",
    "print(\"\\n[Info MyRainPrediction] Model Logistic Regression - AUC(tr) = \" + str(trauc) + \" - AUC(tt) = \" + str(ttauc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-afghanistan",
   "metadata": {},
   "source": [
    "# Parte 2 del Ejercicio: Modelo de predicción para Canberra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-opportunity",
   "metadata": {},
   "source": [
    "Lectura del daset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "sophisticated-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark\\\n",
    ".read\\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"delimiter\", \",\")\\\n",
    ".option(\"inferSchema\", True)\\\n",
    ".load(\"rain_tomorrow_in_australia_mod.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dietary-louis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- PXW: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "victorian-hungarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Location='Cairns'),\n",
       " Row(Location='NorfolkIsland'),\n",
       " Row(Location='Bendigo'),\n",
       " Row(Location='Walpole'),\n",
       " Row(Location='Canberra'),\n",
       " Row(Location='Woomera'),\n",
       " Row(Location='Adelaide'),\n",
       " Row(Location='Cobar'),\n",
       " Row(Location='SydneyAirport'),\n",
       " Row(Location='Goulburn'),\n",
       " Row(Location='PerthAirport'),\n",
       " Row(Location='Wollongong'),\n",
       " Row(Location='Williamtown'),\n",
       " Row(Location='Moree'),\n",
       " Row(Location='Mildura'),\n",
       " Row(Location='Portland'),\n",
       " Row(Location='Albany'),\n",
       " Row(Location='SalmonGums'),\n",
       " Row(Location='Brisbane'),\n",
       " Row(Location='Sydney'),\n",
       " Row(Location='Perth'),\n",
       " Row(Location='Sale'),\n",
       " Row(Location='BadgerysCreek'),\n",
       " Row(Location='Hobart'),\n",
       " Row(Location='Tuggeranong'),\n",
       " Row(Location='Ballarat'),\n",
       " Row(Location='GoldCoast'),\n",
       " Row(Location='MelbourneAirport'),\n",
       " Row(Location='Dartmoor'),\n",
       " Row(Location='Nhil'),\n",
       " Row(Location='PearceRAAF'),\n",
       " Row(Location='Albury'),\n",
       " Row(Location='Yass'),\n",
       " Row(Location='Witchcliffe'),\n",
       " Row(Location='WaggaWagga'),\n",
       " Row(Location='Queanbeyan'),\n",
       " Row(Location='Darwin'),\n",
       " Row(Location='Uluru'),\n",
       " Row(Location='Nuriootpa'),\n",
       " Row(Location='CoffsHarbour'),\n",
       " Row(Location='Melbourne'),\n",
       " Row(Location='Penrith'),\n",
       " Row(Location='MountGambier'),\n",
       " Row(Location='NorahHead'),\n",
       " Row(Location='Katherine'),\n",
       " Row(Location='MountGinini'),\n",
       " Row(Location='Townsville'),\n",
       " Row(Location='Newcastle'),\n",
       " Row(Location='AliceSprings'),\n",
       " Row(Location='Watsonia'),\n",
       " Row(Location='Richmond'),\n",
       " Row(Location='Launceston')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select('Location').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-admission",
   "metadata": {},
   "source": [
    "Filtramos las localizaciones que nos interesan para realizar el modelo de Canberra (Tal como se nos indicó, al tener pocos registros Canberra hemos de coger también los de Yass y Queanbeyan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dependent-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= df2.filter( (df2.Location  == \"Canberra\") | (df2.Location  == \"Yass\") | (df2.Location  == \"Queanbeyan\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "exact-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|  Location|count(1)|\n",
      "+----------+--------+\n",
      "|  Canberra|      68|\n",
      "|      Yass|     500|\n",
      "|Queanbeyan|    2100|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df3.groupBy('Location').agg(count('*')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-quality",
   "metadata": {},
   "source": [
    "####  Realizamos la limpieza del dataset:\n",
    "    - Eliminamos las columnas que nos se han de tener en cuenta en el modelo\n",
    "    - Rellenamos los nulos de las columnas double con la media\n",
    "    - Rellenamos los nulos de las columnas string con \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "heavy-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- PXW: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df3 = df3.drop(col('Date')).drop(col('Location')).drop(col('RISK_MM'))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dutch-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = fill_with_mean(df3, [\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\", \"RainToday\", \"RainTomorrow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "neutral-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.na.fill(value=\"unknown\",subset=[\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\", \"RainToday\", \"RainTomorrow\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-boring",
   "metadata": {},
   "source": [
    "Comprobamos que en todas las variables los nulos = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "falling-aruba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = false)\n",
      " |-- MaxTemp: double (nullable = false)\n",
      " |-- Rainfall: double (nullable = false)\n",
      " |-- Evaporation: double (nullable = false)\n",
      " |-- Sunshine: double (nullable = false)\n",
      " |-- WindGustDir: string (nullable = false)\n",
      " |-- WindGustSpeed: double (nullable = false)\n",
      " |-- WindDir9am: string (nullable = false)\n",
      " |-- WindDir3pm: string (nullable = false)\n",
      " |-- WindSpeed9am: double (nullable = false)\n",
      " |-- WindSpeed3pm: double (nullable = false)\n",
      " |-- Humidity9am: double (nullable = false)\n",
      " |-- Humidity3pm: double (nullable = false)\n",
      " |-- Pressure9am: double (nullable = false)\n",
      " |-- Pressure3pm: double (nullable = false)\n",
      " |-- Cloud9am: double (nullable = false)\n",
      " |-- Cloud3pm: double (nullable = false)\n",
      " |-- Temp9am: double (nullable = false)\n",
      " |-- Temp3pm: double (nullable = false)\n",
      " |-- RainToday: string (nullable = false)\n",
      " |-- RainTomorrow: string (nullable = false)\n",
      " |-- PXW: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-repository",
   "metadata": {},
   "source": [
    "### Realizamos en Modelo de Random Forest para la zona de Canberra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "exposed-domain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872\n",
      "796\n"
     ]
    }
   ],
   "source": [
    "(tr_set_C, tt_set_C) = df3.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "print(tr_set_C.count())\n",
    "\n",
    "\n",
    "print(tt_set_C.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "secondary-improvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|RainTomorrow|count(1)|\n",
      "+------------+--------+\n",
      "|          No|    1551|\n",
      "|         Yes|     321|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_set_C.groupBy('RainTomorrow').agg(count('*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "wrapped-delay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|RainTomorrow|count(1)|\n",
      "+------------+--------+\n",
      "|          No|     638|\n",
      "|         Yes|     158|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tt_set_C.groupBy('RainTomorrow').agg(count('*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "legislative-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1 = [c[0] for c in df3.dtypes if c[1]=='double']\n",
    "\n",
    "assembler_C = VectorAssembler()\\\n",
    ".setInputCols(feats1)\\\n",
    ".setOutputCol(\"features_C\")\n",
    "\n",
    "# Defining the label/target column\n",
    "\n",
    "label_indexer_C = StringIndexer()\\\n",
    ".setInputCol(\"RainTomorrow\")\\\n",
    ".setOutputCol(\"label_idx_C\")\n",
    "\n",
    "# A random forest as classifier\n",
    "\n",
    "classstage_C = RandomForestClassifier()\\\n",
    "    .setNumTrees(50)\\\n",
    "    .setFeatureSubsetStrategy(\"sqrt\")\\\n",
    "    .setImpurity(\"gini\")\\\n",
    "    .setMaxDepth(16)\\\n",
    "    .setMaxBins(32)\\\n",
    "    .setLabelCol(\"label_idx_C\")\\\n",
    "    .setFeaturesCol(\"features_C\")\\\n",
    "    .setSubsamplingRate(0.7)\\\n",
    "    .setMinInstancesPerNode(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "middle-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pipeline\n",
    "\n",
    "pipeline = Pipeline()\\\n",
    "    .setStages([assembler_C, label_indexer_C, classstage_C])\n",
    "\n",
    " # Training\n",
    "modelC = pipeline.fit(tr_set_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "authentic-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Making preds on test data\n",
    "\n",
    "ttpreds_C = modelC.transform(tt_set_C)\n",
    "\n",
    "trpreds_C = modelC.transform(tr_set_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "interstate-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info MyVoiceDetectionApp] ModelModelo con Random Forest Classifier Canberra - AUC(tr) = 0.9263524085556303 - AUC(tt) = 0.8611860640450779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "# Model evaluator\n",
    "\n",
    "evaluator_C = BinaryClassificationEvaluator()\\\n",
    ".setLabelCol(\"label_idx_C\")\\\n",
    ".setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "ttauc_C = evaluator_C.evaluate(ttpreds_C)\n",
    "\n",
    "trauc_C = evaluator_C.evaluate(trpreds_C)\n",
    "\n",
    "print(\"\\n[Info MyVoiceDetectionApp] ModelModelo con Random Forest Classifier Canberra - AUC(tr) = \" + str(trauc_C) + \" - AUC(tt) = \" + str(ttauc_C) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-walnut",
   "metadata": {},
   "source": [
    "#### En este caso aunque tenemos un punto de partida con pocos datos conseguimos un AUC en TEST de 0,86"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-death",
   "metadata": {},
   "source": [
    "Para cualquier consulta adicional mis contactos son: aranzazu.garcia@live.u-tad.com y también: aranza9991@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
